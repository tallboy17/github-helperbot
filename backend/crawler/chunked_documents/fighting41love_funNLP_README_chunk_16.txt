Repository: fighting41love/funNLP
Language: Python
Stars: 74199
Forks: 14882
-----
| 资源名（Name）      | 描述（Description） | 链接     |
| :---        |    :---  |          :--- |
|BMList|大模型大列表|[github](https://github.com/OpenBMB/BMList)|
| bert论文中文翻译     |        | [link](https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation)   |
|    bert原作者的slides  |    |  [link](https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ)  |
| 文本分类实践     |        |  [github](https://github.com/NLPScott/bert-Chinese-classification-task)  |
|  bert tutorial文本分类教程     |        | [github](https://github.com/Socialbird-AILab/BERT-Classification-Tutorial) |
| bert pytorch实现       |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |
|   bert pytorch实现      |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |
|  BERT生成句向量，BERT做文本分类、文本相似度计算     |        | [github](https://github.com/terrifyzhao/bert-utils)   |
|  bert、ELMO的图解     |        |  [github](https://jalammargithubio/illustrated-bert/)  |
|  BERT Pre-trained models and downstream applications     |        |  [github](https://github.com/asyml/texar/tree/master/examples/bert)  |
|  语言/知识表示工具BERT & ERNIE      |        |   [github](https://github.com/PaddlePaddle/LARK)  |
|    Kashgari中使用gpt-2语言模型    |        |  [github](https://github.com/BrikerMan/Kashgari)   |
|     Facebook LAMA   |    用于分析预训练语言模型中包含的事实和常识知识的探针。语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口    |  [github](https://github.com/facebookresearch/LAMA)   |
|    中文的GPT2训练代码    |        |    [github](https://github.com/Morizeyao/GPT2-Chinese) |
|   XLMFacebook的跨语言预训练语言模型     |        |   [github](https://github.com/facebookresearch/XLM)  |
|    海量中文预训练ALBERT模型    |        |  [github](https://github.com/brightmart/albert_zh)   |
|    Transformers 20    |    支持TensorFlow 20 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言   | [github](https://github.com/huggingface/transformers)    |
|    8篇论文梳理BERT相关模型进展与反思    |        |    [github](https://wwwmsracn/zh-cn/news/features/bert) |
|    法文RoBERTa预训练语言模型    |    用138GB语料训练的法文RoBERTa预训练语言模型    |   [link](https://camembert-model.fr/)  |
|     中文预训练 ELECTREA 模型    |    基于对抗学习 pretrain Chinese Model    |   [github](https://github.com/CLUEbenchmark/ELECTRA)  |
|   albert-chinese-ner     |   用预训练语言模型ALBERT做中文NER    |   [github](https://github.com/ProHiryu/albert-chinese-ner)  |
|    开源预训练语言模型合集    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |
|   中文ELECTRA预训练模型     |        |  [github](https://github.com/ymcui/Chinese-ELECTRA)   |
|    用Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)预测下一个词(模型比较)    |        |  [github](https://github.com/renatoviolin/next_word_prediction)   |
|   TensorFlow Hub     |    40+种语言的新语言模型(包括中文)    |  [link](https://tfhub.dev/google/collections/wiki40b-lm/1)   |
|   UER     | 基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）       |  [github](https://github.com/dbiir/UER-py)    |
|    开源预训练语言模型合集    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |
|   多语言句向量包     |        |  [github](https://github.com/yannvgn/laserembeddings)   |
|Language Model as a Service (LMaaS)|语言模型即服务|[github](https://github.com/txsun1997/LMaaS-Papers)|
|开源语言模型GPT-NeoX-20B|200亿参数，是目前最大的可公开访问的预训练通用自回归语言模型|[github](https://github.com/EleutherAI/gpt-neox)|
|中文科学文献数据集（CSL）|包含 396,209 篇中文核心期刊论文元信息 （标题、摘要、关键词、学科、门类）。CSL 数据集可以作为预训练语料，也可以构建许多NLP任务，例如文本摘要（标题预测）、 关键词生成和文本分类等。|[github](https://github.com/ydli-ai/CSL)|
|大模型开发神器||[github](https://github.com/hpcaitech/ColossalAI)|