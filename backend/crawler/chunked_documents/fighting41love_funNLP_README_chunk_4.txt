Repository: fighting41love/funNLP
Language: Python
Stars: 74199
Forks: 14882
-----
| 资源名（Name）      | 描述（Description） | 链接     |
| :---        |    :----   |          :--- |
|  Open LLMs：可供商业使用的开放大型语言模型(LLM)    |    A list of open LLMs available for commercial use   |   [github](https://github.com/eugeneyan/open-llms)    |
|LLM Zoo: 大型语言模型的数据、模型和基准集市|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|[github](https://github.com/FreedomIntelligence/LLMZoo)|
|   大型语言模型(LLM)资料合集   |     相关论文列表，包括指导、推理、决策、持续改进和自我提升等方面的研究工作  |    [LLM资料合集](https://github.com/floodsung/LLM-with-RL-papers)   |
|DecryptPrompt|总结Prompt&LLM论文，开源数据&模型，AIGC应用|[github](https://github.com/DSXiangLi/DecryptPrompt)|
|   SmartGPT    |   旨在为大型语言模型(尤其是GPT-3.5和GPT-4)提供完成复杂任务的能力，通过将它们分解成更小的问题，并使用互联网和其他外部来源收集信息。特点包括模块化设计，易于配置，以及对插件的高度支持。SmartGPT的运作基于"Autos"的概念，包括"Runner"和"Assistant"两种类型，都配有处理计划、推理和任务执行的LLM代理。此外，SmartGPT还具有内存管理系统，以及可以定义各种命令的插件系统     |  [github-SmartGPT](https://github.com/Cormanz/smartgpt)  |
| OpenGPT      |   用于创建基于指令的数据集并训练对话领域专家大型语言模型(LLMs)的框架。已经成功应用于训练健康护理对话模型NHS-LLM，利用来自英国国家卫生服务体系(NHS)网站的数据，生成了大量的问答对和独特对话  |          [github-OpenGPT](https://github.com/CogStack/OpenGPT) |
|   PaLM 2技术报告   |   Google最新发布PaLM 2，一种新的语言模型，具有更好的多语言和推理能力，同时比其前身PaLM更节省计算资源。PaLM 2综合了多项研究进展，包括计算最优的模型和数据规模、更多样化和多语言的数据集、以及更有效的模型架构和目标函数。PaLM 2在多种任务和能力上达到了最先进的性能，包括语言水平考试、分类和问答、推理、编程、翻译和自然语言生成等。PaLM 2还展示了强大的多语言能力，能够处理数百种语言，并在不同语言之间进行翻译和解释。PaLM 2还考虑了负责任的使用问题，包括推理时控制毒性、减少记忆化、评估潜在的伤害和偏见等    |    [PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)   |
|   DB-GPT   |    于vicuna-13b和FastChat的开源实验项目，采用了langchain和llama-index技术进行上下文学习和问答。项目完全本地化部署，保证数据的隐私安全，能直接连接到私有数据库处理私有数据。其功能包括SQL生成、SQL诊断、数据库知识问答等   |   [github-DB-GPT](https://github.com/csunny/DB-GPT)     |
|  Transformers相关文献资源大列表    |   包含了各种各样的Transformer模型，例如BERT、GPT、Transformer-XL等，这些模型已经在许多自然语言处理任务中得到了广泛应用。此外，该列表还提供了这些模型的相关论文和代码链接，为自然语言处理领域的研究人员和开发者提供了很好的参考资源    |   [github](https://github.com/abacaj/awesome-transformers)    |
|   GPT-4终极指南   |    一份关于如何使用GPT3和GPT4的指南，其中包括100多个资源，可以帮助学习如何用它来提高生活效率。包括如何学习ChatGPT基础知识、如何学习ChatGPT高级知识、如何在语言学习中使用GPT-3、如何在教学中使用GPT-3、如何使用GPT-4等，还提供了如何升级到ChatGPT+计划以使用GPT-4以及如何免费使用GPT-4的方法等内容。同时，还提供了如何在业务、生产力、受益、金钱等方面使用ChatGPT的指南   |   [link](https://doc.clickup.com/37456139/d/h/13q28b-324/e2a22b0c164b1f9)    |
|  基于LoRA的LLM参数高效微调    |       |   [link](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html)    |
|  复杂推理：大语言模型的北极星能力     |  在 GPT-4 发布博客中，作者写道：“在一次随意的谈话中，GPT-3.5 和 GPT-4 之间的区别可能是微妙的。当任务的复杂程度达到足够的阈值时，差异就会显现出来。”这意味着复杂任务很可能是大型和小型语言模型的关键差异因素。在这篇文章中，我们将仔细分析讨论如何让大语言模型拥有强大的复杂推理能力。     |   [blog](https://yaofu.notion.site/6dafe3f8d11445ca9dcf8a2ca1c5b199)    |
|   大型语言模型的涌现能力是否是海市蜃楼？   |    大语言模型的涌现能力一直是被大家视作很神奇的现象，似乎是一种大力出奇迹，但这篇论文认为这可能只是一种错觉。   |   [paper](https://arxiv.org/abs/2304.15004)    |
|   大语言模型的概率总结  |   非常详尽的LLM科学解释和总结    |   [paper](https://wangxinyilinda.github.io/pdf/MAE_online.pdf)    |
|  LLaMA 模型简史    |    LLaMA是Meta发布的语言模型，采用Transformer架构，有多个版本，最大为65B参数。与GPT类似，可用于进一步微调，适用于多种任务。与GPT不同的是，LLaMA是开源的，可以在本地运行。现有的LLaMA模型包括：Alpaca、Vicuna、Koala、GPT4-x-Alpaca和WizardLM。每个模型都有不同的训练数据和性能表现   |   [blog](https://agi-sphere.com/llama-models/)    |
|  大型语言模型的复杂推理     |   讨论了如何训练具有强大复杂推理能力的语言模型，并探讨了如何有效地提示模型以充分释放其潜力；针对语言模型和编程的训练相似性，提出了三阶段的训练：持续训练、监督微调和强化学习；介绍了评估大型语言模型推理能力的一套任务集合；讨论了如何进行提示工程，通过提供各种学习机会使模型获得更好的学习效果，最终实现智能化    |   [link](https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75)    |
|   大语言模型进化树   |       |    [paper](https://arxiv.org/pdf/2304.13712.pdf)   |
|李宏毅：穷人如何低资源复刻自己的ChatGPT||[blog](https://mp.weixin.qq.com/s/GAFYwlqY2SoTlCW7b4kOyA)|
|   训练ChatGPT的必备资源：语料、模型和代码库完全指南   |       |    [资源链接](https://github.com/RUCAIBox/LLMSurvey)[论文地址](https://arxiv.org/pdf/2303.18223.pdf)   |
|  GitHub宝藏库，里面整理了GPT相关的各种开源项目    |       |    [github](https://github.com/EwingYangs/awesome-open-gpt)   |
|  ChatGPT中文指南    |       |   [gitlab](https://gitlab.com/awesomeai/awesome-chatgpt-zh)    |
|   探讨了ChatGPT在自然语言处理中的应用、优势、限制以及未来发展方向   |   强调了在使用该技术时的伦理道德考量和提示工程技术。    |    [paper](https://arxiv.org/abs/2304.02017)   |
|大型语言模型相关文献资源列表||[github](https://github.com/RUCAIBox/LLMSurvey)|
|大型语言模型文献综述--中文版||[github](https://github.com/fighting41love/funNLP/tree/master/data/paper/LLM_Survey_Chinese_0418.pdf)|
|ChatGPT 相关资源大列表||[github](https://github.com/OpenMindClub/awesome-chatgpt)|
|Pre-Training to Learn in Context||[paper](https://arxiv.org/abs/2305.09137)|
|Langchain架构图||[image](https://pbs.twimg.com/media/Fv4hst2aIAAKypt?format=jpg&name=4096x4096)|
|LLM开发人员都应该知道的数字||[github](https://github.com/ray-project/llm-numbers)|
|大语言模型如何构建强大的复杂推理能力||[blog](https://zhuanlan.zhihu.com/p/626533715)|
|LLMs九层妖塔|分享打怪(ChatGLM、Chinese-LLaMA-Alpaca、MiniGPT-4、FastChat、LLaMA、gpt4all等)实战与经验|[github](https://github.com/km1994/LLMsNineStoryDemonTower)|