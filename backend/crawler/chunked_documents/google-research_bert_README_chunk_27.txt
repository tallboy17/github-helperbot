Repository: google-research/bert
Language: Python
Stars: 39243
Forks: 9676
-----
For now, cite [the Arxiv paper](https://arxiv.org/abs/1810.04805):  
```
@article{devlin2018bert,
title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
journal={arXiv preprint arXiv:1810.04805},
year={2018}
}
```  
If we submit the paper to a conference or journal, we will update the BibTeX.