Repository: hpcaitech/ColossalAI
Language: Python
Stars: 40976
Forks: 4521
-----
Colossal-AI provides a collection of parallel components for you. We aim to support you to write your
distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart
distributed training and inference in a few lines.  
- Parallelism strategies
- Data Parallelism
- Pipeline Parallelism
- 1D, [2D](https://arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D](https://arxiv.org/abs/2105.14450) Tensor Parallelism
- [Sequence Parallelism](https://arxiv.org/abs/2105.13120)
- [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)
- [Auto-Parallelism](https://arxiv.org/abs/2302.02599)  
- Heterogeneous Memory Management
- [PatrickStar](https://arxiv.org/abs/2108.05818)  
- Friendly Usage
- Parallelism based on the configuration file  
<p align="right">(<a href="#top">back to top</a>)</p>